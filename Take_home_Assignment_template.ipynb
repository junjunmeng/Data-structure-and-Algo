{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8o2n3WYQt58yYvBcqLmjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junjunmeng/Data-structure-and-Algo/blob/main/Take_home_Assignment_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvgkgLnDPtqT"
      },
      "source": [
        "Table of Contents\n",
        "\n",
        "*  Initial Data Analysis\n",
        "*  Data Wrangling\n",
        "*  Exploratory Data Analysis\n",
        "*  Statistical Analysis\n",
        "*  Machine Learning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miW34qATIUKt"
      },
      "source": [
        "pip install etsy_py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlMPDYHYHvCI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "01b1cfee-0901-486d-d9ff-f6d479cb90a2"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "pip install --upgrade pandas_profiling\n",
        "import pandas_profiling as pp\n",
        "# import etsy_py\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "import sklearn as sklearn\n",
        "from sklearn.utils import resample\n",
        "from sklearn import preprocessing as preprocessing\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import statsmodels.api as sm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7tcmvIdKjdp"
      },
      "source": [
        "# <font color = 'blue'> 1. Data Clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2snzLq2HvyM"
      },
      "source": [
        "#### Initial Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emy6H_3oj4Bn"
      },
      "source": [
        "def initial_analysis(df):\n",
        "    \"\"\"\n",
        "    Given a dataframe produces a simple report on initial data analytics\n",
        "    Params:\n",
        "        - df \n",
        "    Returns:\n",
        "        - Shape of dataframe records and columns\n",
        "        - Columns and data types\n",
        "    \"\"\"\n",
        "    print('Report of Initial Data Analysis:\\n')\n",
        "    print(f'Shape of dataframe: {df.shape}')\n",
        "    print(f'Features and Data Types: \\n {df.dtypes}')\n",
        "    print(\"DataFrame Row Number: \", df.shape[0])\n",
        "    print(\"Unique IDs: \", df.ID.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpdHv51gHzhe"
      },
      "source": [
        "#### Percentage of missing value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orF9TRY3j4Xx"
      },
      "source": [
        "def percent_missing(df):\n",
        "    \"\"\"\n",
        "    Given a dataframe it calculates the percentage of missing records per column\n",
        "    Params:\n",
        "        - df\n",
        "    Returns:\n",
        "        - Dictionary of column name and percentage of missing records\n",
        "    \"\"\"\n",
        "    col=list(df.columns)\n",
        "    perc=[round(df[c].isna().mean()*100,2) for c in col]\n",
        "    miss_dict=dict(zip(col,perc))\n",
        "    return miss_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sR8OYybH3b_"
      },
      "source": [
        "#### Missing value exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3TPKQOQwP_e"
      },
      "source": [
        "# missing values in a column, especially response variable\n",
        "def missing_class(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, it calculates the count of missing records\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - col_name : col_name\n",
        "    Returns:\n",
        "        - number of records\n",
        "  \"\"\"\n",
        "  missing_vals = [\"No idea\", np.nan, \"#\", \"?\"]\n",
        "  print(\"Number of missing rows within \" + col_name +\" :\"  + str(df[col_name].isnull().sum(axis=0)))\n",
        "\n",
        "  # replace missing or abnormal value with np.nan\n",
        "  df[col_name] = df[col_name].replace(missing_vals, np.nan)\n",
        "\n",
        "  # dropping na\n",
        "  df.dropna(subset = [col_name], inplace=True)\n",
        "  \n",
        "  # check the cleaned colname\n",
        "  print(df.groupby(col_name).size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq66FiLTC15p"
      },
      "source": [
        "#### Converting data types for selected columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojU2US0ry1vM"
      },
      "source": [
        "# covert dataframe columns to numeric or other types\n",
        "\n",
        "def convert_num(df, columns):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, it convert to another data type\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - columns : list of columns\n",
        "    Returns:\n",
        "        - dataframe with converted data type\n",
        "  \"\"\"\n",
        "  for i in range(0, len(columns)):\n",
        "    df[columns[i]] = df[columns[i]].astype(\"int64\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOx9_LWrDAiM"
      },
      "source": [
        "#### Duplicate check, basically two situations\n",
        "1. duplicated identical rows\n",
        "2. same ID with different values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9l0reNuDGMR"
      },
      "source": [
        "# any duplicate rows? especially in ID\n",
        "\n",
        "def duplicate_row_remove(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, remove duplicated rows\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - col_name(str): a column name, e.g \"ID\"\n",
        "    Returns:\n",
        "        - number of remaining rows\n",
        "  \"\"\"\n",
        "  global breast # dataframe name\n",
        "  # original length of dataframe\n",
        "  original_length = len(df)\n",
        "  # Number of unique IDs\n",
        "  print(\"Number of unique IDs: \" + str(df[col_name].nunique()))\n",
        "  # remove duplicated rows\n",
        "  df = df[~df.loc[:, col_name:].duplicated()]\n",
        "\n",
        "  # new length of dataframe\n",
        "  new_length = len(df)\n",
        "  # count of rows removed\n",
        "  rows_removed = original_length - new_length\n",
        "  print(\"Number of identical replicated rows should removed: \" + str(rows_removed))\n",
        "  print(\"Number of remaining rows: \" + str(len(df)))\n",
        "  breast = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReQFVhSNJ11M"
      },
      "source": [
        "# same user ID with different values\n",
        "\n",
        "def duplicate_ID(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, remove non-identical rows with same ID\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - col_name: a column name, e.g ID\n",
        "    Returns:\n",
        "        - number of remaining rows\n",
        "  \"\"\"\n",
        "  global breast # dataframe name\n",
        "  ID_dup = df[df[col_name].duplicated()]\n",
        "  print(\"Number of duplicated ID: \" + str(len(ID_dup)))\n",
        "\n",
        "  # keep the smallest index for each user\n",
        "  df = df.sort_values(by= [col_name, \"Index\"], ascending = True)\n",
        "  df = df[~df[col_name].duplicated(keep = 'first')]\n",
        "  print(\"Number of remaining rows: \" + str(len(df)))\n",
        "  breast = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzY4Q7oxKZ4D"
      },
      "source": [
        "#### Check and remove out-ranged Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_XumbPXKajF"
      },
      "source": [
        "def Incorrect_feature_values(df,columns, val, col_name ):\n",
        "  \"\"\"\n",
        "      Given a dataframe and value, remove col_name's value > val\n",
        "    Params:\n",
        "        - df(dataframe): dataframe\n",
        "        - col_name (str): a column name, e.g \"ID\"\n",
        "        - val(int): specific value \n",
        "        - columns(list): columns list that need to check the value range\n",
        "    Returns: dataframe removed rows with out-ranged value\n",
        "  \"\"\"\n",
        "  global breast\n",
        "  rows_with_large_vals = []\n",
        "  for col in range(0, len(columns)):\n",
        "    filter_by_col = df[columns[col]] > val\n",
        "    ID_vals = pd.array(df[filter_by_col][col_name])\n",
        "    rows_with_large_vals.append(ID_vals)\n",
        "\n",
        "  # group list of lists into 1 list\n",
        "  flat_list = []\n",
        "  for sublist in rows_with_large_vals:\n",
        "    for item in sublist:\n",
        "      flat_list.append(item)\n",
        "  \n",
        "  # list of users with out range values\n",
        "  users_with_large_vals = pd.array(flat_list).unique()\n",
        "  print(\"There are \"+ str(len(users_with_large_vals)) + \" users with values that exceed \" + str(val))\n",
        "\n",
        "  # remove record with any out-range values\n",
        "  df = df[~df[col_name].isin(pd.array(users_with_large_vals))]\n",
        "  breast = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD1TGPkKEF_Q"
      },
      "source": [
        "#### Re-assign value to a column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBngui01j_FM"
      },
      "source": [
        "# rename the class based on code\n",
        "def reClass(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and col_name, re-assign value according to multiple conditions\n",
        "    Params:\n",
        "        - df(dataframe): dataframe\n",
        "        - col_name (str): a column name, e.g \"ID\"\n",
        "    Returns: dataframe's column with re-assigned value\n",
        "  \"\"\"\n",
        "  global breast\n",
        "  class_new = []\n",
        "  for item in df[col_name]:\n",
        "    if item == 2:\n",
        "      class_new.append(0)\n",
        "    elif item ==  4:\n",
        "      class_new.append(1)\n",
        "    else:\n",
        "      class_new.append(np.nan)\n",
        "  #df.drop(df_col)\n",
        "  df['Class_new'] = class_new\n",
        "  # drop original column\n",
        "  df = df.drop(col_name, axis = 1)\n",
        "  # assign new column as original column name\n",
        "  df = df.rename(columns = {'Class_new': col_name})\n",
        "  # remove NA\n",
        "  df = df[df[col_name] != np.nan]\n",
        "  breast = df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0DSs1gFoSy"
      },
      "source": [
        "#### Aggregation summary of each columns in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWmhLA1Ij_Lc"
      },
      "source": [
        "# check the aggregation for each column\n",
        "def col_agg(df, col_list):\n",
        "  col_summary = []\n",
        "  for name in col_list:\n",
        "    col_summary.append(df.groupby(name).size().reindex())\n",
        "  return col_summary\n",
        "\n",
        "# eg.  col_agg(breast, breast.columns[1:12])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAJyFmTYKu2b"
      },
      "source": [
        "# <font color = 'blue'> 2. Data Exploratory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKiGcO0BZicF"
      },
      "source": [
        "#### Set Profile Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRsiXgP3ZphO"
      },
      "source": [
        "def profile(df):\n",
        "\n",
        "  \"\"\"\n",
        "    Given a dataframe, return data profile \n",
        "    Params:\n",
        "      - df(dataframe): dataframe\n",
        "    Returns: data profile in html format\n",
        "   \"\"\"\n",
        "  global prof\n",
        "  from pandas_profiling import ProfileReport\n",
        "  prof = ProfileReport(df)\n",
        "  prof.to_file(output_file= \"data_profile.html\")\n",
        "  return prof"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtmDYtMYxeoz"
      },
      "source": [
        "#### Count plot in Seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3zqpqupxeOX"
      },
      "source": [
        "def sub_countplot(df, Class):\n",
        "  \"\"\"\n",
        "    Given a dataframe, return data profile \n",
        "    Params:\n",
        "      - df(dataframe): dataframe\n",
        "      - Class(str): col_name, e.g \"Class\", usually response variable\n",
        "    Returns: count_plot by features\n",
        "  \"\"\" \n",
        "  features = df.columns[1:-1].to_list()\n",
        "  feature_num = len(features)\n",
        "  x = 3\n",
        "  y = feature_num//3 + feature_num%3\n",
        "  fig, ax = plt.subplots(x, y, figsize= (15,15))\n",
        "  for i in range(feature_num):\n",
        "    sns.countplot(x= features[i], hue= Class, data = df, ax = ax[i//3, i%3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2M6naRGHm5X"
      },
      "source": [
        "#### Check normality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1pH6czkE8g"
      },
      "source": [
        "def normality_test(df,col_list):\n",
        "    \"\"\"\n",
        "    Given a dataframe determines whether each numerical column is Gaussian \n",
        "    H0 = Assumes distribution is not Gaussian\n",
        "    Ha = Assumes distribution is Gaussian\n",
        "    Params:\n",
        "        - df\n",
        "    Returns:\n",
        "        - W Statistic\n",
        "        - p-value\n",
        "        - List of columns that do not have gaussian distribution\n",
        "    \"\"\"\n",
        "    non_gauss=[]\n",
        "    w_stat=[]\n",
        "    # Determine if each sample of numerical feature is gaussian\n",
        "    alpha = 0.05\n",
        "    for n in col_list:\n",
        "        stat,p=shapiro(df[n])\n",
        "        print(sns.distplot(df[n]))\n",
        "        print(n, \"skew is:\", skew(df[n]), \"kurtosis is :\", kurtosis(df[n]))\n",
        "\n",
        "\n",
        "        if p <= alpha: # Reject Ho -- Distribution is not normal\n",
        "            non_gauss.append(n)\n",
        "            w_stat.append(stat)\n",
        "    # Dictionary of numerical features not gaussian and W-Statistic        \n",
        "    norm_dict=dict(zip(non_gauss,w_stat))\n",
        "    return norm_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OgVV_Q8BzCR"
      },
      "source": [
        "#### Outliers by Boxplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8e-W3oXkH74"
      },
      "source": [
        "# Outliers by boxplot\n",
        "col_names = breast.columns[2:11]\n",
        "col_names\n",
        "\n",
        "breast.boxplot(column=['Clump Thickness', 'Uniformity of Cell Size',\n",
        "       'Uniformity of Cell Shape', 'Marginal Adhesion',\n",
        "       'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
        "       'Normal Nucleoli','Mitoses' ], grid=False, rot=90, fontsize=11)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go3_x8lwB31J"
      },
      "source": [
        "#### Data transformation by boxcox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDYh_0JkKXx"
      },
      "source": [
        "# data transformation?\n",
        "from scipy.stats import boxcox\n",
        "plt.hist(boxcox(breast['Clump Thickness'],1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSpmgkzTB9Q8"
      },
      "source": [
        "#### Visualize the binary classes count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG0o_fCRkNJr"
      },
      "source": [
        "def class_fig(col):\n",
        "  \"\"\"\n",
        "  Given the response variable, it visulize the count in binary classes\n",
        "  Params:\n",
        "    - Series\n",
        "  Returns:\n",
        "    - countplot\n",
        "  \"\"\"\n",
        "  sns.countplot(col, label = 'Count')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGgrsBJHkNcd"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGcHugGUCDJH"
      },
      "source": [
        "#### Correlation heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNRi0KYykNfa"
      },
      "source": [
        "# correlation between features\n",
        "\n",
        "def corr_heatmap(df, col_names):\n",
        "  corr = df[col_names].corr()\n",
        "  mask = np.zeros_like(corr)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  sns.heatmap(corr, annot= True, vmin= -1, vmax= 1, mask= mask )\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkEaiHBxCHa5"
      },
      "source": [
        "#### Feature Importance by Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUkTGiD-kNmK"
      },
      "source": [
        "# feature importance by Random Forest\n",
        "\n",
        "def rf_importance(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3, random_state = 42)\n",
        "  rf = RandomForestClassifier()\n",
        "  rf.fit(X_train,y_train)\n",
        "  \n",
        "  # Feature Importance\n",
        "  importance_rf = pd.Series(rf.feature_importances_, index = X_train.columns)\n",
        "  sorted_importance_rf = importance_rf.sort_values()\n",
        "  sorted_importance_rf.plot(kind = 'barh', color = 'lightgreen')\n",
        "  plt.title(\"Feature Importance by Random Forest\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne3lEQ_HLLNT"
      },
      "source": [
        "# <font color = 'blue'> 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGHWjHx7CMRT"
      },
      "source": [
        "#### SMOTE for resampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXLEk6UdkVAh"
      },
      "source": [
        "# SMOTE for Upsampling training \n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split( breast[breast.columns[0:8]], breast['Class'], test_size= 0.3, random_state = 42)\n",
        "X_res, y_res = SMOTE(random_state = 42).fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsfUdfnXLadG"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEl4zGuBLYF9"
      },
      "source": [
        "X_train = pd.DataFrame(preprocessing.normalize(X_train))\n",
        "X_test = pd.DataFrame(preprocessing.normalize(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7AMXt8mMHlm"
      },
      "source": [
        "#### Standardlization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUmnQdG3MEwq"
      },
      "source": [
        "# standardlization\n",
        "\n",
        "ss = StandardScaler()\n",
        "train_X = ss.fit_transform(train_X)\n",
        "test_X = ss.fit_transform(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJdn-WXSCQ9Y"
      },
      "source": [
        "#### Logistic Regression with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyO5g6L3kW1h"
      },
      "source": [
        "# Logistic Regression \n",
        "\n",
        "def LogReg(X_res, y_res, X_test, y_test):\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  from sklearn.metrics import roc_curve\n",
        "  logreg = LogisticRegression().fit(X_res, y_res)\n",
        "  y_pred = logreg.predict(X_test)\n",
        "  print(\"confusion_matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "  print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "  print(\"ROC:\", roc_auc_score(y_test, y_pred))\n",
        "\n",
        "  # compute predicted probabilites: y_pred_prob\n",
        "  y_pred_prob = logreg.predict_log_proba(X_test)[:, 1]\n",
        "\n",
        "  # generate ROC curve\n",
        "\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "  # plot ROC curve\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--')\n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('ROC Curve')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_5js3yiMzVn"
      },
      "source": [
        "#### Logistic Regression with statsmodels.api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF8ILE7bMl_h"
      },
      "source": [
        "def logit_SM(X_train, y_train):\n",
        "  import statsmodels.api as sm\n",
        "  logit_model = sm.Logit(y_train.values.ravel(), X_train)\n",
        "  result = logit_model.fit()\n",
        "  print(result.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtcp9RwSCTon"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMOViXgpkY8Z"
      },
      "source": [
        "# random Forest\n",
        "def RF_pipe(X_res, y_res, X_test, y_test):\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  from sklearn.metrics import roc_curve\n",
        "  rf = RandomForestClassifier().fit(X_res, y_res)\n",
        "  y_pred = rf.predict(X_test)\n",
        "  print(\"confusion_matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "  print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "  print(\"ROC:\", roc_auc_score(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjPzmCijOrbU"
      },
      "source": [
        "# <font color = 'green'> Example 1: Conversion rate\n",
        "The goal of this challenge is to build a model that predicts conversion rate and, based on the model, come up with ideas to improve it.\n",
        "\n",
        "We have data about all users who hit our site: whether they converted or not as well as some of their characteristics such as their country, the marketing channel, their age, whether they are repeat users and the number of pages visited during that session (as a proxy for site activity/time spent on site).\n",
        "\n",
        "Your project is to:\n",
        "\n",
        "1.   Predict conversion rate\n",
        "2.   Come up with recommendations for the product team and the marketing team to improve conversion rate\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZmLzoIWO1Kp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtkTo5rTQ1a"
      },
      "source": [
        "## Data Exploration with Seaborn\n",
        "[Seaborn Gallary](https://seaborn.pydata.org/examples/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfHhM6H_T72s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}